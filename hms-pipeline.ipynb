{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training Pipeline\n","\n","All dataloading and training in one place!"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-14T05:18:43.921801Z","iopub.status.busy":"2024-04-14T05:18:43.921227Z","iopub.status.idle":"2024-04-14T05:18:52.582521Z","shell.execute_reply":"2024-04-14T05:18:52.581567Z","shell.execute_reply.started":"2024-04-14T05:18:43.921765Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from sklearn.model_selection import KFold, GroupKFold, train_test_split\n","from datetime import datetime\n","from tqdm.notebook import tqdm\n","import albumentations as A\n","import matplotlib.pyplot as plt\n","import os\n","from typing import List, Tuple, Union\n","import pickle\n","from lightning.pytorch.loggers.tensorboard import TensorBoardLogger\n","from torcheval.metrics import MulticlassAccuracy\n","from torcheval.metrics.functional import multiclass_f1_score\n","\n","#Local packages\n","from src import CNNDetector, ContrastiveDetector\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\"\"\"\n","Configure\n","\n","Change the paths of the data directory to the location of your HMS Dataset.\n","The sub directories of TRAIN/TEST_EEG, TRAIN/TEST_SPEC should remain the same\n","\n","\"\"\"\n","\n","#Kaggle\n","# DATA_ROOT = \"/kaggle/input/hms-harmful-brain-activity-classification/\"\n","# TRAIN_EEG = \"train_eegs\"\n","# TRAIN_SPEC = \"train_spectrograms\"\n","# TEST_EEG = \"test_eegs\"\n","# TEST_SPEC = \"test_spectrograms\"\n","\n","#Local\n","DATA_ROOT = \"/home/benluo/HBAC/data/hbac\"\n","TRAIN_EEG = \"train_eegs\"\n","TRAIN_SPEC = \"train_spectrograms\"\n","TEST_EEG = \"test_eegs\"\n","TEST_SPEC = \"test_spectrograms\"\n","\n","#Other hyperparams\n","VALID_FOLDS = 5"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T05:18:52.584370Z","iopub.status.busy":"2024-04-14T05:18:52.583950Z","iopub.status.idle":"2024-04-14T05:18:52.616226Z","shell.execute_reply":"2024-04-14T05:18:52.615329Z","shell.execute_reply.started":"2024-04-14T05:18:52.584344Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Reduce train dataset size\n","\n","The code below only takes the first portion of data in the training list to reduce the number of spectrogram and EEG samples in the training dataset\n","\n","You can choose not to run the following cell if `train_reduced.csv` already exists"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train_reduced.csv already exists\n"]}],"source":["#Get reduced train list as the entire dataset is heavy on RAM, \n","\n","reduce_train_path = os.path.join(DATA_ROOT, \"train_reduced.csv\")\n","\n","if os.path.isfile(reduce_train_path):\n","\n","    print(\"train_reduced.csv already exists\")\n","\n","else:\n","\n","    train_list = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n","\n","    reduced_len = train_list.shape[0]//2 #32 GB of RAM during training\n","    # reduced_len = train_list.shape[0]//4 #Less RAM but less data\n","\n","    train_list_reduced = train_list.iloc[:reduced_len,:]\n","\n","    train_list_reduced.to_csv(os.path.join(DATA_ROOT, \"train_reduced.csv\"))"]},{"cell_type":"markdown","metadata":{},"source":["### Generate Data\n","\n","Since test data does not come with labels, for local training and testing, we will only use the data from `train.csv`\n","\n","The Dataset follows a train-val-test split.\n","\n","The train and test datasets are split first by `test_size`.\n","\n","The train and validation datasets are split by Group K-Folds into `val_folds` uniform groups. One group will be chosen randomly for validation while the rest will be used for training every epoch."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def generate_data(val_folds:int = VALID_FOLDS,\n","                  test_size:float = 0.1,\n","                  saved_spec:str = \"all_spec.pkl\",\n","                  saved_eeg:str = \"all_eeg.pkl\"):\n","\n","    #Use reduced csv file\n","    train_list = pd.read_csv(os.path.join(DATA_ROOT, \"train_reduced.csv\"))\n","\n","    print(\"All data\", train_list.shape)\n","    display(train_list.head())\n","\n","    label_cols = train_list.columns[-6:]\n","\n","    #Create new df to be formated for training and testing\n","    train_df = train_list[['spectrogram_id','eeg_id','patient_id','spectrogram_label_offset_seconds','eeg_label_offset_seconds']]\n","\n","    #Normalise labels into probabilities\n","    aux = train_list[label_cols].copy()\n","    \n","    for label in label_cols:\n","        train_df[label] = aux[label].values\n","        \n","    y_data = train_df[label_cols].values\n","    y_data = y_data / y_data.sum(axis=1,keepdims=True)\n","    train_df[label_cols] = y_data\n","\n","    #Target label/class\n","    aux = train_list['expert_consensus'].copy()\n","    train_df['target'] = aux\n","    train_df = train_df.reset_index()\n","\n","    #Sort df by patient id so testing and training data will be less similar\n","    train_df = train_df.sort_values(\"patient_id\")\n","\n","    test_df = train_df.iloc[int((1-test_size)*train_df.shape[0]):]\n","\n","    train_df = train_df.iloc[:int((1-test_size)*train_df.shape[0])]\n","\n","    train_df = train_df.reset_index()\n","    test_df = test_df.reset_index()\n","\n","    print(\"Training data\")\n","    display(train_df.head())\n","\n","    print(\"Testing data\")\n","    display(test_df.head())\n","\n","\n","    gkf = GroupKFold(n_splits=val_folds)\n","\n","    #KFold grouped by patient id\n","    for fold, (train_index, val_index) in enumerate(gkf.split(train_df, train_df.target, train_df.patient_id)):\n","        train_df.loc[val_index, \"fold\"] = int(fold)\n","    \n","    all_eeg = {}\n","    \n","    all_spec = {}\n","\n","    #Try loading eeg and spec data if they have been generated previously\n","    #If not, read eeg and spec data from train_list and save the collection in data folder\n","    #Data is saved as float16 due to space constraints\n","    try:\n","\n","        with open(os.path.join(DATA_ROOT,saved_spec), \"rb\") as handle:\n","            all_spec = pickle.load(handle)\n","\n","        with open(os.path.join(DATA_ROOT,saved_eeg), \"rb\") as handle:\n","            all_eeg = pickle.load(handle)\n","\n","    except:\n","    \n","        for idx, row in tqdm(train_list.iterrows()):\n","\n","            spec_id = row[\"spectrogram_id\"]\n","            eeg_id = row[\"eeg_id\"]\n","\n","            if spec_id not in all_spec:\n","\n","                spec = pd.read_parquet(os.path.join(DATA_ROOT, TRAIN_SPEC, str(spec_id)+\".parquet\"))\n","\n","                all_spec[spec_id] = spec.iloc[:,1:].values.astype(dtype=np.float32)\n","\n","            if eeg_id not in all_eeg:\n","\n","                eeg = pd.read_parquet(os.path.join(DATA_ROOT, TRAIN_EEG, str(eeg_id)+\".parquet\"))\n","\n","                all_eeg[eeg_id] = eeg.iloc[:,1:].values.astype(dtype=np.float32)\n","        \n","        with open(os.path.join(DATA_ROOT, saved_eeg), \"wb\") as handle:\n","            pickle.dump(all_eeg, handle)\n","        \n","        with open(os.path.join(DATA_ROOT, saved_spec), \"wb\") as handle:\n","            pickle.dump(all_spec, handle)\n","    \n","    return train_df, test_df, all_eeg, all_spec, label_cols"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["All data (53400, 16)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>eeg_id</th>\n","      <th>eeg_sub_id</th>\n","      <th>eeg_label_offset_seconds</th>\n","      <th>spectrogram_id</th>\n","      <th>spectrogram_sub_id</th>\n","      <th>spectrogram_label_offset_seconds</th>\n","      <th>label_id</th>\n","      <th>patient_id</th>\n","      <th>expert_consensus</th>\n","      <th>seizure_vote</th>\n","      <th>lpd_vote</th>\n","      <th>gpd_vote</th>\n","      <th>lrda_vote</th>\n","      <th>grda_vote</th>\n","      <th>other_vote</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1628180742</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>353733</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>127492639</td>\n","      <td>42516</td>\n","      <td>Seizure</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1628180742</td>\n","      <td>1</td>\n","      <td>6.0</td>\n","      <td>353733</td>\n","      <td>1</td>\n","      <td>6.0</td>\n","      <td>3887563113</td>\n","      <td>42516</td>\n","      <td>Seizure</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1628180742</td>\n","      <td>2</td>\n","      <td>8.0</td>\n","      <td>353733</td>\n","      <td>2</td>\n","      <td>8.0</td>\n","      <td>1142670488</td>\n","      <td>42516</td>\n","      <td>Seizure</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1628180742</td>\n","      <td>3</td>\n","      <td>18.0</td>\n","      <td>353733</td>\n","      <td>3</td>\n","      <td>18.0</td>\n","      <td>2718991173</td>\n","      <td>42516</td>\n","      <td>Seizure</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1628180742</td>\n","      <td>4</td>\n","      <td>24.0</td>\n","      <td>353733</td>\n","      <td>4</td>\n","      <td>24.0</td>\n","      <td>3080632009</td>\n","      <td>42516</td>\n","      <td>Seizure</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0      eeg_id  eeg_sub_id  eeg_label_offset_seconds  \\\n","0           0  1628180742           0                       0.0   \n","1           1  1628180742           1                       6.0   \n","2           2  1628180742           2                       8.0   \n","3           3  1628180742           3                      18.0   \n","4           4  1628180742           4                      24.0   \n","\n","   spectrogram_id  spectrogram_sub_id  spectrogram_label_offset_seconds  \\\n","0          353733                   0                               0.0   \n","1          353733                   1                               6.0   \n","2          353733                   2                               8.0   \n","3          353733                   3                              18.0   \n","4          353733                   4                              24.0   \n","\n","     label_id  patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  \\\n","0   127492639       42516          Seizure             3         0         0   \n","1  3887563113       42516          Seizure             3         0         0   \n","2  1142670488       42516          Seizure             3         0         0   \n","3  2718991173       42516          Seizure             3         0         0   \n","4  3080632009       42516          Seizure             3         0         0   \n","\n","   lrda_vote  grda_vote  other_vote  \n","0          0          0           0  \n","1          0          0           0  \n","2          0          0           0  \n","3          0          0           0  \n","4          0          0           0  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training data\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_173713/3136675821.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  train_df[label] = aux[label].values\n","/tmp/ipykernel_173713/3136675821.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  train_df[label] = aux[label].values\n","/tmp/ipykernel_173713/3136675821.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  train_df[label] = aux[label].values\n","/tmp/ipykernel_173713/3136675821.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  train_df[label] = aux[label].values\n","/tmp/ipykernel_173713/3136675821.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  train_df[label] = aux[label].values\n","/tmp/ipykernel_173713/3136675821.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  train_df[label] = aux[label].values\n","/tmp/ipykernel_173713/3136675821.py:25: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  train_df[label_cols] = y_data\n","/tmp/ipykernel_173713/3136675821.py:29: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  train_df['target'] = aux\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>level_0</th>\n","      <th>index</th>\n","      <th>spectrogram_id</th>\n","      <th>eeg_id</th>\n","      <th>patient_id</th>\n","      <th>spectrogram_label_offset_seconds</th>\n","      <th>eeg_label_offset_seconds</th>\n","      <th>seizure_vote</th>\n","      <th>lpd_vote</th>\n","      <th>gpd_vote</th>\n","      <th>lrda_vote</th>\n","      <th>grda_vote</th>\n","      <th>other_vote</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>49755</td>\n","      <td>49755</td>\n","      <td>971557517</td>\n","      <td>1506484236</td>\n","      <td>56</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>31423</td>\n","      <td>31423</td>\n","      <td>610627902</td>\n","      <td>1085571772</td>\n","      <td>56</td>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>23898</td>\n","      <td>23898</td>\n","      <td>466986202</td>\n","      <td>3685996567</td>\n","      <td>56</td>\n","      <td>1008.0</td>\n","      <td>40.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>23897</td>\n","      <td>23897</td>\n","      <td>466986202</td>\n","      <td>3685996567</td>\n","      <td>56</td>\n","      <td>968.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>47440</td>\n","      <td>47440</td>\n","      <td>925581305</td>\n","      <td>271569269</td>\n","      <td>56</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>Other</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   level_0  index  spectrogram_id      eeg_id  patient_id  \\\n","0    49755  49755       971557517  1506484236          56   \n","1    31423  31423       610627902  1085571772          56   \n","2    23898  23898       466986202  3685996567          56   \n","3    23897  23897       466986202  3685996567          56   \n","4    47440  47440       925581305   271569269          56   \n","\n","   spectrogram_label_offset_seconds  eeg_label_offset_seconds  seizure_vote  \\\n","0                               0.0                       0.0           0.0   \n","1                              10.0                      10.0           0.0   \n","2                            1008.0                      40.0           0.0   \n","3                             968.0                       0.0           0.0   \n","4                               0.0                       0.0           0.0   \n","\n","   lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote target  \n","0       0.0       0.0        0.0        0.0         1.0  Other  \n","1       0.0       0.0        0.0        0.0         1.0  Other  \n","2       0.0       0.0        0.0        0.0         1.0  Other  \n","3       0.0       0.0        0.0        0.0         1.0  Other  \n","4       0.0       0.0        0.0        0.0         1.0  Other  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Testing data\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>level_0</th>\n","      <th>index</th>\n","      <th>spectrogram_id</th>\n","      <th>eeg_id</th>\n","      <th>patient_id</th>\n","      <th>spectrogram_label_offset_seconds</th>\n","      <th>eeg_label_offset_seconds</th>\n","      <th>seizure_vote</th>\n","      <th>lpd_vote</th>\n","      <th>gpd_vote</th>\n","      <th>lrda_vote</th>\n","      <th>grda_vote</th>\n","      <th>other_vote</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>35879</td>\n","      <td>35879</td>\n","      <td>707621855</td>\n","      <td>1933039126</td>\n","      <td>57272</td>\n","      <td>8.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>LRDA</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5367</td>\n","      <td>5367</td>\n","      <td>91996359</td>\n","      <td>3686473557</td>\n","      <td>57272</td>\n","      <td>12.0</td>\n","      <td>12.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>LRDA</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5391</td>\n","      <td>5391</td>\n","      <td>91996359</td>\n","      <td>3686473557</td>\n","      <td>57272</td>\n","      <td>76.0</td>\n","      <td>76.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>LRDA</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5390</td>\n","      <td>5390</td>\n","      <td>91996359</td>\n","      <td>3686473557</td>\n","      <td>57272</td>\n","      <td>74.0</td>\n","      <td>74.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>LRDA</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5373</td>\n","      <td>5373</td>\n","      <td>91996359</td>\n","      <td>3686473557</td>\n","      <td>57272</td>\n","      <td>28.0</td>\n","      <td>28.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>LRDA</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   level_0  index  spectrogram_id      eeg_id  patient_id  \\\n","0    35879  35879       707621855  1933039126       57272   \n","1     5367   5367        91996359  3686473557       57272   \n","2     5391   5391        91996359  3686473557       57272   \n","3     5390   5390        91996359  3686473557       57272   \n","4     5373   5373        91996359  3686473557       57272   \n","\n","   spectrogram_label_offset_seconds  eeg_label_offset_seconds  seizure_vote  \\\n","0                               8.0                       8.0           0.0   \n","1                              12.0                      12.0           0.0   \n","2                              76.0                      76.0           0.0   \n","3                              74.0                      74.0           0.0   \n","4                              28.0                      28.0           0.0   \n","\n","   lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote target  \n","0       0.0       0.0        1.0        0.0         0.0   LRDA  \n","1       0.0       0.0        1.0        0.0         0.0   LRDA  \n","2       0.0       0.0        1.0        0.0         0.0   LRDA  \n","3       0.0       0.0        1.0        0.0         0.0   LRDA  \n","4       0.0       0.0        1.0        0.0         0.0   LRDA  "]},"metadata":{},"output_type":"display_data"}],"source":["train_df, test_df, all_eeg, all_spec, label_cols = generate_data()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T05:37:19.304302Z","iopub.status.busy":"2024-04-14T05:37:19.303853Z","iopub.status.idle":"2024-04-14T05:37:19.335251Z","shell.execute_reply":"2024-04-14T05:37:19.334018Z","shell.execute_reply.started":"2024-04-14T05:37:19.304270Z"},"trusted":true},"outputs":[],"source":["class HMSDataset(Dataset):\n","\n","    def __init__(self,\n","                 df:pd.DataFrame = None,\n","                 aug: bool = False) -> None:\n","        \n","\n","        super(HMSDataset, self).__init__()\n","\n","        self.df = df\n","\n","        self.eeg_sample_freq = 200 # 200 Hz\n","        self.spec_sample_freq = 0.5 # 0.5 Hz\n","    \n","        \n","        #data augmentation\n","        self.aug = aug\n","        self.transforms = A.Compose([\n","            A.HorizontalFlip(p=0.5)\n","        ])\n","\n","    def format_data(self, eeg:np.array, spec:np.array) -> Tuple[torch.tensor]:\n","\n","        #Epsilon for numerical stability during division (prevent division by zero)\n","        eps = 1e-6\n","\n","        #Convert data from saved float16 to float32 during training and testing\n","        eeg = eeg.astype(dtype=np.float32)\n","        spec = spec.astype(dtype=np.float32)\n","\n","        #Normalising and getting rid of Nans\n","        eeg_mean = np.nanmean(eeg.flatten())\n","        eeg_std = np.nanstd(eeg.flatten())\n","        eeg = (eeg-eeg_mean)/(eeg_std+eps)\n","        eeg = np.nan_to_num(eeg, nan=0.0)\n","\n","        #Limiting range of spec data\n","        spec = np.clip(spec, np.exp(-4), np.exp(8))\n","        spec = np.log(spec)\n","        \n","        #Normalising and getting rid of Nans\n","        spec_mean = np.nanmean(spec.flatten())\n","        spec_std = np.nanstd(spec.flatten())\n","        spec = (spec-spec_mean)/(spec_std+eps)\n","        spec = np.nan_to_num(spec, nan=0.0)\n","        \n","        #If data augmentation\n","        if self.aug:\n","\n","            eeg = self.transforms(image=eeg)[\"image\"]\n","            spec = self.transforms(image=spec)[\"image\"]\n","\n","        #Convert to tensors\n","        eeg = torch.tensor(eeg.copy())\n","        spec = torch.tensor(spec.copy())\n","\n","        return eeg, spec\n","\n","\n","    def __getitem__(self, index) -> dict:\n","    \n","        row = self.df.iloc[index]\n","\n","        eeg_id = row[\"eeg_id\"]\n","        spec_id = row[\"spectrogram_id\"]\n","\n","        #EEG Sub-sampling\n","        start = int(row[\"eeg_label_offset_seconds\"]*self.eeg_sample_freq)\n","        end = int((row[\"eeg_label_offset_seconds\"]+50)*self.eeg_sample_freq)\n","        eeg = all_eeg[eeg_id][start:end].T\n","        \n","        #Spectrogram Sub-sampling\n","        start = int(row[\"spectrogram_label_offset_seconds\"]*self.spec_sample_freq)\n","        end = int((row[\"spectrogram_label_offset_seconds\"]+600)*self.spec_sample_freq)\n","        spec = all_spec[spec_id][start:end].T\n","\n","        #Normalizing and getting rid of Nans\n","        eeg, spec = self.format_data(eeg, spec)\n","\n","        #Convert label to tensor\n","        label = torch.tensor(row[label_cols], dtype=torch.float32)\n","\n","        return eeg, spec, label\n","\n","\n","    def __len__(self):\n","\n","        return self.df.shape[0]\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","\n","        eeg, spec, label = zip(*batch)\n","        \n","        if eeg is not None:\n","            eeg = torch.stack(eeg, dim=0).float()\n","        \n","        if spec is not None:\n","            spec = torch.stack(spec, dim=0).float().unsqueeze(1).expand(-1,3,-1,-1)\n","\n","        if label is not None:            \n","            label = torch.stack(label, dim=0)\n","        \n","        return {\n","            \"eeg\": eeg,\n","            \"spec\": spec,\n","            \"label\": label\n","        }\n","    "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[torch.Size([19, 10000]), torch.Size([400, 300]), torch.Size([6])]\n","(tensor([[ 2.3614,  2.3194,  2.4823,  ..., -1.7455, -1.6031, -1.6775],\n","        [-0.3700, -0.2605, -0.1144,  ..., -2.2169, -2.3075, -2.4801],\n","        [ 0.0102,  0.2174,  0.3497,  ..., -0.6527, -0.7875, -0.9105],\n","        ...,\n","        [-0.0809, -0.0693, -0.6586,  ...,  0.1719, -0.3792, -0.2762],\n","        [-0.3010, -0.1300, -0.2249,  ...,  0.6544,  0.4266,  0.3476],\n","        [ 0.0321, -0.1489, -0.3506,  ..., -0.0801, -0.3042, -0.0270]]), tensor([[ 0.3566,  0.4194,  0.5765,  ...,  1.3942,  0.8542,  1.0623],\n","        [ 0.6647,  0.5925,  0.9843,  ...,  1.4696,  1.0413,  1.1379],\n","        [ 0.8955,  0.7546,  1.0413,  ...,  1.4917,  1.0032,  1.2002],\n","        ...,\n","        [-0.5482, -0.5900, -0.4620,  ..., -0.8459, -1.1279, -1.4918],\n","        [-0.6362, -0.5900, -0.3586,  ..., -0.9148, -1.1597, -1.1941],\n","        [-0.5175, -0.4072, -0.4188,  ..., -1.0199, -1.0705, -1.0982]]), tensor([0., 0., 0., 1., 0., 0.]))\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_173713/839579866.py:81: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  label = torch.tensor(row[label_cols], dtype=torch.float32)\n"]}],"source":["#Check dataset object is working\n","\n","test_dataset = HMSDataset(df=test_df)\n","\n","print([data.shape for data in test_dataset[0]])\n","\n","print(test_dataset[0])"]},{"cell_type":"markdown","metadata":{},"source":["### Model\n","\n","Load your model here, the current models are supported:\n","\n","1. CNN\n","    - ConvNext\n","    - EfficientNet b0\n","    - EfficientNet v2s\n","2. Custom Architecture\n","    - Contrastive Detector"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ContrastiveDetector(\n","  (kl_loss): KLDivLoss()\n","  (backbone_eeg): Sequential(\n","    (0): Conv1d(19, 64, kernel_size=(5,), stride=(5,), bias=False)\n","    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Conv1d(64, 128, kernel_size=(7,), stride=(5,), bias=False)\n","    (4): Sequential(\n","      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (1): ReLU()\n","      (2): Conv1d(128, 128, kernel_size=(5,), stride=(3,), bias=False)\n","    )\n","    (5): Sequential(\n","      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (1): ReLU()\n","      (2): Conv1d(128, 128, kernel_size=(5,), stride=(3,), bias=False)\n","    )\n","    (6): Sequential(\n","      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (1): ReLU()\n","      (2): Conv1d(128, 128, kernel_size=(5,), stride=(3,), bias=False)\n","    )\n","    (7): Sequential(\n","      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (1): ReLU()\n","      (2): Conv1d(128, 256, kernel_size=(5,), stride=(3,), bias=False)\n","    )\n","    (8): Sequential(\n","      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (1): ReLU()\n","      (2): Conv1d(256, 512, kernel_size=(3,), stride=(1,), bias=False)\n","    )\n","  )\n","  (backbone_spec): EfficientNet(\n","    (features): Sequential(\n","      (0): Conv2dNormActivation(\n","        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (1): Sequential(\n","        (0): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (2): Conv2dNormActivation(\n","              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n","        )\n","      )\n","      (2): Sequential(\n","        (0): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n","              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n","        )\n","        (1): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n","              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n","        )\n","      )\n","      (3): Sequential(\n","        (0): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n","              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n","        )\n","        (1): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n","              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n","        )\n","      )\n","      (4): Sequential(\n","        (0): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n","              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n","        )\n","        (1): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n","              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n","        )\n","        (2): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n","              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n","        )\n","      )\n","      (5): Sequential(\n","        (0): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n","              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n","        )\n","        (1): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n","              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n","        )\n","        (2): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n","              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n","        )\n","      )\n","      (6): Sequential(\n","        (0): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n","              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n","        )\n","        (1): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n","              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n","        )\n","        (2): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n","              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n","        )\n","        (3): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n","              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n","        )\n","      )\n","      (7): Sequential(\n","        (0): MBConv(\n","          (block): Sequential(\n","            (0): Conv2dNormActivation(\n","              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (1): Conv2dNormActivation(\n","              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n","              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): SiLU(inplace=True)\n","            )\n","            (2): SqueezeExcitation(\n","              (avgpool): AdaptiveAvgPool2d(output_size=1)\n","              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n","              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n","              (activation): SiLU(inplace=True)\n","              (scale_activation): Sigmoid()\n","            )\n","            (3): Conv2dNormActivation(\n","              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            )\n","          )\n","          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n","        )\n","      )\n","      (8): Conv2dNormActivation(\n","        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=1)\n","    (classifier): Sequential(\n","      (0): Dropout(p=0.2, inplace=True)\n","      (1): Linear(in_features=1280, out_features=512, bias=True)\n","    )\n","  )\n","  (norm_eeg): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (norm_spec): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (head): Sequential(\n","    (0): Linear(in_features=1024, out_features=256, bias=False)\n","    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Linear(in_features=256, out_features=6, bias=False)\n","  )\n","  (output_layer): Softmax(dim=1)\n",")\n"]}],"source":["model_config = {\n","    \"model_name\": \"efficientnet_b0\",\n","    \"num_features\": 512,\n","    \"num_classes\": 6,\n","}\n","\n","# model = CNNDetector(model_config).to(device)\n","model = ContrastiveDetector(model_config).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Training and Testing\n","\n","Validation is done every `test_step` to check if model is overfitting on training data.\n","\n","Testing is done at the end of every epoch with the model produced by that epoch.\n","\n","Training and testing metrics can be viewed in tensorboard as mentioned in the README."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T02:39:35.310349Z","iopub.status.busy":"2024-04-14T02:39:35.309988Z","iopub.status.idle":"2024-04-14T02:39:35.324053Z","shell.execute_reply":"2024-04-14T02:39:35.322936Z","shell.execute_reply.started":"2024-04-14T02:39:35.310321Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def validate(model, valid_dataloader):\n","\n","    model.eval()\n","\n","    loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n","\n","    loss = torch.tensor([0.]).to(device)\n","\n","    for batch in tqdm(valid_dataloader):\n","\n","        for key in batch:\n","\n","            batch[key] = batch[key].to(device)\n","\n","        predictions = model(batch)\n","        \n","        predictions = F.log_softmax(predictions,dim=1)\n","\n","        loss += loss_fn(predictions,batch[\"label\"])\n","\n","    loss = loss / len(valid_dataloader)\n","\n","    model.train()\n","\n","    print(\"Validation loss\", loss.item())\n","\n","    return loss.item()\n","\n","@torch.no_grad()\n","def test(model, test_dataloader):\n","\n","    model.eval()\n","\n","    loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n","\n","    num_classes = 6\n","\n","    acc = MulticlassAccuracy(average=\"macro\", num_classes=num_classes)\n","    f1_score = torch.tensor([0.]).to(device)\n","    loss = torch.tensor([0.]).to(device)\n","     \n","    for batch in tqdm(test_dataloader):\n","\n","        for key in batch:\n","            batch[key] = batch[key].to(device)\n","\n","        predictions = model(batch)\n","\n","        loss += loss_fn(F.log_softmax(predictions, dim=1), batch[\"label\"])\n","\n","        predictions = F.softmax(predictions, dim=1)\n","\n","        predictions = torch.argmax(predictions, dim=1)\n","\n","        classes = torch.argmax(batch[\"label\"], dim=1)\n","\n","        f1_score += multiclass_f1_score(predictions, classes, num_classes=num_classes)\n","\n","        acc.update(predictions, classes)\n","\n","    model.train()\n","\n","    f1_score /= len(test_dataloader)\n","    loss /= len(test_dataloader)\n","\n","    return acc.compute(), f1_score, loss\n","\n","#training\n","\n","def train_epoch(model=None,\n","          train_dataloader=None,\n","          valid_dataloader=None,\n","          test_dataloader=None,\n","          optimiser = None,\n","          train_config=None,\n","          valid_config=None,\n","          lr_scheduler=None,\n","          min_valid_loss=100.,\n","          min_test_loss=100.,\n","          model_name=\"model\",\n","          epoch=0,\n","          logger=None):\n","    \n","    \"\"\"\n","    Training Function\n","    \"\"\"\n","\n","    assert(train_dataloader is not None)\n","    \n","    assert(model is not None)\n","\n","    #Training Params\n","\n","    save_model = train_config.get(\"save_model\", True)\n","    to_model_keys = [\"spec\", \"eeg\", \"label\"]\n","    valid_step = valid_config.get(\"valid_step\", 1000)\n","    verbose_step = train_config.get(\"verbose_step\", 10)\n","    \n","    \n","    loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n","    \n","    model.train()\n","    \n","    for itr, batch in tqdm(enumerate(train_dataloader)):\n","\n","#       print(batch[\"eeg\"].shape)\n","\n","        #Train One Iteration\n","\n","        #Load data to GPU\n","\n","        for key in to_model_keys:\n","\n","            batch[key] = batch[key].to(device)\n","\n","        predictions = model(batch)\n","\n","        if \"contrastive\" in model_name:\n","\n","            klloss, contrastive_loss = model.get_loss(predictions, batch[\"label\"])\n","\n","            loss = klloss+contrastive_loss\n","        \n","        else:\n","            \n","            loss = model.get_loss(predictions,batch[\"label\"])\n","        \n","        optimiser.zero_grad()\n","\n","        loss.backward()\n","\n","        optimiser.step()\n","        \n","        if lr_scheduler is not None:\n","            lr_scheduler.step()\n","        \n","        if itr%verbose_step==0:\n","            print(f\"Training itr {itr}/{len(train_dataloader)}\")\n","            for param_group in optimiser.param_groups:\n","                lr = param_group['lr']\n","                break\n","            \n","\n","            if \"contrastive\" in model_name:\n","                print(\"Training KL Loss: \", klloss.item(), \"Training Contrastive Loss: \", contrastive_loss.item(), \"Learning Rate:\", lr)\n","                logger.log_metrics({\"train/klloss\":klloss, \"train/contrastiveloss\": contrastive_loss, \"train/lr\": lr}, itr+epoch*len(train_dataloader))\n","            else:\n","                print(\"Training Loss: \", loss.item(), \"Learning Rate:\", lr)\n","                logger.log_metrics({\"train/loss\":loss, \"train/lr\": lr}, itr+epoch*len(train_dataloader))\n","        \n","        #Validation\n","        \n","        if itr%valid_step==0 and itr>0 and valid_dataloader is not None:\n","            print(\"Validation\")\n","            valid_loss = validate(model, valid_dataloader)\n","            \n","            if valid_loss < min_valid_loss and save_model:\n","                min_valid_loss = valid_loss\n","                save_path = os.path.join(DATA_ROOT,\"models\")\n","                os.makedirs(save_path, exist_ok=True)\n","                torch.save(model.state_dict(), os.path.join(save_path, f\"{model_name}_best.pt\"))\n","            logger.log_metrics({\"valid/loss\":valid_loss}, itr+epoch*len(train_dataloader))\n","        \n","    #Test\n","    if test_dataloader is not None:\n","        acc, f1_score, loss = test(model, test_dataloader)\n","        if loss < min_test_loss:\n","            min_test_loss = loss\n","        logger.log_metrics({\"test/acc\":acc, \"test/f1_score\":f1_score, \"test/loss\":loss}, itr+epoch*len(train_dataloader))\n","    return min_valid_loss, min_test_loss"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def train(model=None,\n","          optimiser=None,\n","          lr_scheduler=None,\n","          train_config=None,\n","          valid_config=None,\n","          test_dataloader=None,\n","          model_name=\"model\",\n","          logger=None):\n","    \n","    train_batch_size = train_config.get(\"batch_size\", 32)\n","    num_epochs = train_config.get(\"num_epochs\", 10)\n","    valid_folds = train_config.get(\"valid_folds\", 5)\n","    valid_batch_size = valid_config.get(\"batch_size\", 32)\n","    train_workers = train_config.get(\"workers\", 1)\n","    valid_workers = valid_config.get(\"workers\", 1)\n","\n","    min_valid_loss = 100.\n","    min_test_loss = 100.\n","\n","    for epoch in range(num_epochs):\n","\n","        print(\"Epoch\", epoch)\n","\n","        #KGFolds\n","        fold = np.random.randint(valid_folds)\n","        train_df_fold = train_df[train_df[\"fold\"]!=fold]\n","        valid_df = train_df[train_df[\"fold\"]==fold]\n","        train_dataset = HMSDataset(train_df_fold, aug=True)\n","        valid_dataset = HMSDataset(valid_df, aug=False)\n","\n","        train_dataloader = DataLoader(\n","            dataset=train_dataset,\n","            batch_size = train_batch_size,\n","            shuffle=True,\n","            num_workers=train_workers,\n","            collate_fn=train_dataset.collate_fn\n","        )\n","\n","        valid_dataloader = DataLoader(\n","            dataset=valid_dataset,\n","            batch_size = valid_batch_size,\n","            shuffle=True,\n","            num_workers=valid_workers,\n","            collate_fn=valid_dataset.collate_fn\n","        )\n","\n","        min_valid_loss, min_test_loss = train_epoch(model=model,\n","                    train_dataloader=train_dataloader,\n","                    valid_dataloader=valid_dataloader,\n","                    test_dataloader=test_dataloader,\n","                    optimiser=optimiser,\n","                    train_config=train_config,\n","                    valid_config=valid_config,\n","                    lr_scheduler=lr_scheduler,\n","                    min_valid_loss=min_valid_loss,\n","                    min_test_loss=min_test_loss,\n","                    model_name=model_name,\n","                    epoch=epoch,\n","                    logger=logger)\n","        print(\"Min Valid Loss\", min_valid_loss, \"Min Test Loss\", min_test_loss)"]},{"cell_type":"markdown","metadata":{},"source":["#### Hyperparameters\n","\n","Configuration of hyperparamters for training and validation/testing is shown below.\n","\n","If you would like to train the contrastive detector, set `model_name` to \"contrastive_{backbone_name}\", e.g. \"contrastive_efficientnetb0\". Otherwise, Set `model_name` to \"cnn_detector_{backbone_name}\".\n","\n","- `batch_size`: size of one mini-batch of data per forward pass of the model during training, validation and testing respectively\n","- `lr`: learning rate, depends on optimiser and learning rate scheduler\n","- `save_model`: saves the best model determined by validation loss\n","- `valid_folds`: set during dataset geneartion, the number of groups to have in Group KFolds training\n","- `verbose_step`: iteration step interval to print and log training metrics\n","- `valid_step`: iteration step interval to perform validation on model and save best model by validation loss\n","- `workers`: number of CPU workers to load dataset for training, validation, and testing. A higher number requires more RAM but may speed up training"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T02:39:35.801324Z","iopub.status.busy":"2024-04-14T02:39:35.800510Z","iopub.status.idle":"2024-04-14T02:56:53.096301Z","shell.execute_reply":"2024-04-14T02:56:53.094732Z","shell.execute_reply.started":"2024-04-14T02:39:35.801292Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6fef2d1f798e46b882cf11cf731a9642","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_173713/839579866.py:81: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  label = torch.tensor(row[label_cols], dtype=torch.float32)\n"]},{"name":"stdout","output_type":"stream","text":["Training itr 0/1202\n","Training KL Loss:  1.417924165725708 Training Contrastive Loss:  0.004667256027460098 Learning Rate: 4.0000718651559374e-05\n"]},{"name":"stderr","output_type":"stream","text":["/home/benluo/miniconda3/envs/hbac_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n","  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n","/home/benluo/miniconda3/envs/hbac_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1741: RuntimeWarning: invalid value encountered in subtract\n","  np.subtract(arr, avg, out=arr, casting='unsafe', where=where)\n","/tmp/ipykernel_173713/839579866.py:34: RuntimeWarning: invalid value encountered in subtract\n","  eeg = (eeg-eeg_mean)/(eeg_std+eps)\n"]},{"name":"stdout","output_type":"stream","text":["Training itr 100/1202\n","Training KL Loss:  1.0651350021362305 Training Contrastive Loss:  -0.001512644812464714 Learning Rate: 4.731232458808946e-05\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 52\u001b[0m\n\u001b[1;32m     44\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     45\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     46\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mtest_batch_size,\n\u001b[1;32m     47\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mtest_dataset\u001b[38;5;241m.\u001b[39mcollate_fn\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalid_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[11], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimiser, lr_scheduler, train_config, valid_config, test_dataloader, model_name, logger)\u001b[0m\n\u001b[1;32m     31\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     32\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     33\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mcollate_fn\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m valid_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     40\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mvalid_dataset,\n\u001b[1;32m     41\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m valid_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mvalid_dataset\u001b[38;5;241m.\u001b[39mcollate_fn\n\u001b[1;32m     45\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m min_valid_loss, min_test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalid_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_valid_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_valid_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_test_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_test_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin Valid Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_valid_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin Test Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_test_loss)\n","Cell \u001b[0;32mIn[10], line 117\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_dataloader, valid_dataloader, test_dataloader, optimiser, train_config, valid_config, lr_scheduler, min_valid_loss, min_test_loss, model_name, epoch, logger)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m to_model_keys:\n\u001b[1;32m    115\u001b[0m     batch[key] \u001b[38;5;241m=\u001b[39m batch[key]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 117\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontrastive\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[1;32m    121\u001b[0m     klloss, contrastive_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_loss(predictions, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/HBAC/src/contrastive_detector.py:157\u001b[0m, in \u001b[0;36mContrastiveDetector.forward\u001b[0;34m(self, data_dict, inference)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m        returns a dictionary pred_dict with the logits for loss calculation and gradient descent.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m#         print(torch.isnan(torch.sum(data_dict[\"spec\"])))\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m         x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone_eeg(data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    161\u001b[0m         x2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mFlatten()(x2)\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torchvision/models/efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torchvision/models/efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 333\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    336\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torchvision/models/efficientnet.py:164\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    166\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_depth(result)\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/hbac_env/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:137\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m     factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    134\u001b[0m         num_features, eps, momentum, affine, track_running_stats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs\n\u001b[1;32m    135\u001b[0m     )\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input_dim(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_config = {\n","    \"batch_size\": 32,\n","    \"num_epochs\": 5,\n","    \"lr\": 1e-3,\n","    \"save_model\": True,\n","    \"workers\": 1,\n","    \"verbose_step\": 100,\n","    \"weight_decay\": 1e-4,\n","    \"valid_folds\": VALID_FOLDS,\n","}\n","\n","valid_config = {\n","    \"batch_size\": 32,\n","    \"workers\": 1,\n","    \"valid_step\": 500\n","}\n","\n","lr = train_config.get(\"lr\", 1e-3)\n","weight_decay = train_config.get(\"weight_decay\", 0.)\n","model_name = \"contrastive\"\n","num_epochs = train_config.get(\"num_epochs\", 1)\n","folds = train_config.get(\"valid_folds\", 5)\n","train_batch_size = train_config.get(\"batch_size\", 32)\n","test_batch_size = valid_config.get(\"batch_size\", 32)\n","\n","\n","logger = TensorBoardLogger(f\"logs/{model_name}\", name=model_name)\n","\n","optimiser = torch.optim.Adam(\n","model.parameters(),\n","lr=lr,\n","weight_decay=weight_decay\n",")\n","\n","lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","    optimiser,\n","    max_lr=lr,\n","    epochs=num_epochs,\n","    steps_per_epoch=int((folds-1)/folds*train_df.shape[0]/train_batch_size)+10\n",")\n","\n","test_dataset = HMSDataset(df=test_df)\n","\n","test_dataloader = DataLoader(\n","    dataset=test_dataset,\n","    batch_size=test_batch_size,\n","    collate_fn=test_dataset.collate_fn\n",")\n","\n","torch.cuda.empty_cache()\n","\n","train(model=model,\n","      train_config=train_config,\n","      valid_config=valid_config,\n","      test_dataloader=test_dataloader,\n","      model_name=model_name,\n","      optimiser=optimiser,\n","      lr_scheduler=lr_scheduler,\n","      logger=logger)\n","\n","    "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7469972,"sourceId":59093,"sourceType":"competition"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}
